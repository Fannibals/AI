## AI week5 Delete Relaxation Heuristics

### 1. Motivation
+ Delete relaxation is a method to relax planning tasks, and thus automatically compute h.

+ Definition
	- Delete Relaxation
		- a+, when you see the plus, it also means the delete relaxed

	- Relaxed Plan
		- An optimal relaxed plan for s is also that for "sub s plus"

+ Dominance
	- s'+ dominates s+ if s+ belongs to s'+

+ Proposition of the Dominance
	- 1) if s+ is a goal state, s'+ is as well
	- 2) a->+ is applicable in s+, then it is applicable in s'+
	- 3) appl(s,a+) dominates both s and appl(s,a)
	- 4) Delete relaxation is admissible
		- the worst plan that can find, upper bound ???

### 2. Greedy Relaxed Planning
+ Proposition. 
	- Greedy relaxed planning is sound, complete, and terminates in time polynomial in the size of Π.

+ s.t such that
	- preconditions of those actions satisfied by current state
	- and the resulting of that action give me a bigger state
	- include that kind of action to your relaxed plan

+ Using greedy relaxed planning to generate h
	- safe
	- goal-aware
	- no, unless the relaxed plans were optimal

### 3. h+: The Optimal Delete Relaxation Heuristic
+ h+: optimal relax plan cost
+ h\*: optimal cost of the original task
+ h+ is admissible --> safe and goal-aware


+ example:
	- optimal vs. relaxed plan
	- relaxed: direct from sydney to all other cities

+ how to compute h+???
	- Optimal Relaxed Planning
	- PlamOpt+, NP-complete

+ h+ as a relaxation heuristic P18?
	- native relaxation? YES
	- relaxation efficiently constructible? YES
	- relaxation efficiently computable? NO 


### AI- Delete Delaxation continues

#### **1. Question about h+ ??**
+ Manhattan distance is not correct due to relaxed plans cannot walk through walls
+ the answer is h*

#### 2. The Additive and Max heuristics
+ <img src="hmax and hadd" height="400", width="400">
+ h(add):
	- if the goal is already in the state, h=0
	- the minimum cost of going to the goal == min C(actions to goal)
	- 

+ h(max)


+ propositions
	+ h(add) is Pessimistic
		- h+ <= h(add) ： due to sometimes an action will be recorded multiple times.
	+ h(max) is Optimistic
		- h(max) <= h+ <= h*
	+ both are safe

#### 3. Bellman-Ford for h(max) and h(add)
+ 1) create a new table
+ 2) set the first row
+ 3) c(g) => h-value calculation rule
+ 4) f(g) => update rule
+ 5) iteration rule

+ conclusion
	- Both can be quickly computed
	- h(max) is **admissible**, but **far too optimistic**
	- h(add) is not admissible, but is typically a lot more informed that h(max)
	- h(add) is sometimes better than h+
		- overcounts by ignoring positive interactions
		- relaxed plan is a way to deal with overcounts problem

#### 4. Relaxed Plans
+ Calculation
	- First compute a **best-supporter function bs**, 
		- which for every fact p ∈ F returns an action that is deemed to be the cheapest achiever of p (within the relaxation). 
	- Then extract a relaxed plan from that function, by applying it to singleton sub-goals and collecting all the actions.

+ A ∖ B means the set that contains all those elements of A that are not in B.
	- {1,2,3,4} ∖ {3,4,5,6} = {1,2}
+ <img src="relaxed plan" height="400", width="400">

+ <img src="best-supporter" height="400", width="400">

+ **Prerequisite**


+ **h(ff)**
	- a relaxed plan heuristic
	- it returns infinite if no relaxed plan exists  
		,otherwise returns **sum of cost of actions in Rplan** 
+ h(ff) is Pessimistic and Agrees with h* on infinite


















   

